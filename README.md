### kaggle-titanic

Repo for practicing machine learning using the Kaggle Titanic survival prediction competition

* Kaggle score for baseline implementation (no feature engineering, no regularization): 0.39234 (position 11,165)

* Kaggle score for implementation with regularization (no feature engineering): 0.44976 (position 11,162)


**Conclusion**

My goal was to focus on the implementation of logistic regression using
a **cost function** and using the **gradient descent** algorithm.
I am confident that the implementation is correct as it followed the assignemnts of the
[Coursera Machine Learning course by Andrew Ng](https://www.coursera.org/learn/machine-learning).
However, the scores I achieve when submitting on Kaggle are at the lower. The key take away is then, in my view,
that a well-designed solution adf the best algorithms will not perform well unless
considerable effort is put into data exploration and feature engineering!

.

